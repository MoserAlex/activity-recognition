\section{Experiment}
\label{section:experiment}
The focus of this chapter will be the execution of the two experiments. One experiment was focused on collecting acceleration data and then processing and classifying the resulting data sets. Since the second setup used a preexisting data set, its focus instead laid exclusively on the classification of activities.

\subsection{First Experiment}
As previously mentioned in the previous chapter the first prototype is split into two separated programs, each with their own individual tasks and challenges. The goal of the first program is to collect acceleration data and to store it, while the second part is trying to process that data. After extracting various features and training the machine learning algorithm, the second program should be able to classify new data as previously trained activities.

In the first round of gathering data, 5 test subjects were given a mobile phone, a Samsung Galaxy S6. On this phone they launched a web application. The application collected the acceleration data once the test subject started the recording. They had to select which activity they were going to perform and they had six different activities to choose from: walking, running, going upstairs, going downstairs, jumping and standing still. Each activity had to be performed multiple times for approximately 10 seconds each, resulting in a collection of about 4-8 files for each activity per person.

The application recorded acceleration samples at a frequency of 20 Hz and stored it locally on the mobile phone in a \gls{csv} file. Once the recording of an activity was completed, the file was sent to a laptop in the same network. Having to be in the same network made recording data difficult. Some activities, like running for example, were not possible to be performed for an extended period of time, since all tests had to be done indoors. Instead they had to be done in short succession of each other, each time stopping and restarting the recording of the acceleration.

After the collection of the data from one person was completed, the data files were transferred to the second program. This personalized data set was then processed to create feature files. It calculated the energy, Hjorth mobility and Hjorth complexity, each of these features separated into a horizontal and a vertical part. Therefore for further testing six features were available.

This process resulted in one feature file per person in the form of a \gls{csv} file. The structure of these files was set up to contain an integer as its first entry, which corresponds to the activity this row belongs to. The following entries of this row represent a feature vector for this activity.

After creating said feature files, the machine learning algorithms could be trained and tested. This first iteration of a prototype focused solely on using a \gls{svm} for classification. To train this algorithm, the features first needed to be scaled to look standard normally distributed, with a mean of 0 and unit variance. Without this step, the classification algorithm might behave unpredictable. A random collection of scaled features were then picked to train the \gls{svm}. Left out samples could then be used for cross validation and checking how successful the training was. In regard to the research question of this study, 4 feature sets of the 5 test subjects were used to train the \gls{svm}, while the 5$^{th}$ was being classified.

On a larger scale, this would be the approach to generate a generalized data set for activity recognition. First Collect the data of a larger group of training subjects, then train the classification algorithm on their movement patterns. Finally compare it to a new feature set of a test subject, which was not part of the training group.

As already mentioned in chapter \ref{section:technologies} the experiment utilizing the technologies mentioned here was not pursued any further, as too many problems occurred. The encountered issues will be explained in chapter \ref{section:results}.


\subsection{Second Experiment}
In chapter \ref{section:technologies} it was already mentioned that the second prototype utilizes the \gls{scut-naa} data set instead of gathering data itself. This data set contains the acceleration data of 44 test subjects (34 males and 10 females) each performing ten different activities. During their performance the data has been collected by three independent triaxial accelerometer at a frequency of 100Hz.

The three accelerometer have been placed in the subjects trouser pocket, at their waist and in their shirt pocket. Since the sensors have not been fixed to the body, they may have moved randomly in the pockets, which causes more variance in the data. This is not an undesirable effect though, since \textcite[]{xue2010naturalistic} have aimed to produce a more naturalistic set of data.

The next step is calculating the features. The second prototype implements all of the features mentioned in chapter \ref{section:features}: mean, maximum and minimum amplitude, standard deviation, energy in time as well as frequency domain and the 3 Hjorth parameter. All of these features get calculated from the absolute amplitude of the signal, its horizontal component and its vertical component, resulting in a feature vector with 27 different features. These get stored in a feature file, similar to how they have been stored during the first experiment. This saves time when using different combinations of features by not having to calculate them again every time.

Another factor that can have a big impact on the accuracy of the prediction is the cluster size from which the features are calculated. A feature calculated from a larger cluster contains more information about the activity, which will most likely increase to probability of a correct classification. On the downside, the feature set will end up with fewer feature vectors. Since we have less feature vectors to train the algorithm with, this could result in less accuracy. Therefore all features were calculated for 3 different sample sizes, 64, 256 and 512.

Since the goal is to find out which features are best used to train and test for activity recognition, various combinations of features need to be chosen. The prototype is built in a way to easily filter out unwanted features. They then get scaled to look normally distributed with a mean of 0 and unit variance again. The Scikit-learn framework implements a standard scaler, which gets fit to the training set. This scaler can then later be used to apply the same transformation to the test set.

The program does not only use one machine learning algorithm like the previous one to classify activities. It utilizes all of the ones explained in chapter \ref{section:classification}: decision tree, \gls{knn} and \gls{svm} with a linear kernel and \gls{svm} with a \gls{rbf} kernel. Chapter \ref{section:results} highlights the different results and aims to explain them.
